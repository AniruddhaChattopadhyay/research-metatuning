{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>problem</th>\n",
       "      <th>solution</th>\n",
       "      <th>answer</th>\n",
       "      <th>subject</th>\n",
       "      <th>level</th>\n",
       "      <th>unique_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Convert the point $(0,3)$ in rectangular coord...</td>\n",
       "      <td>We have that $r = \\sqrt{0^2 + 3^2} = 3.$  Also...</td>\n",
       "      <td>\\left( 3, \\frac{\\pi}{2} \\right)</td>\n",
       "      <td>Precalculus</td>\n",
       "      <td>2</td>\n",
       "      <td>test/precalculus/807.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Define\\n\\[p = \\sum_{k = 1}^\\infty \\frac{1}{k^2...</td>\n",
       "      <td>We count the number of times $\\frac{1}{n^3}$ a...</td>\n",
       "      <td>p - q</td>\n",
       "      <td>Intermediate Algebra</td>\n",
       "      <td>5</td>\n",
       "      <td>test/intermediate_algebra/1994.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            problem  \\\n",
       "0           0  Convert the point $(0,3)$ in rectangular coord...   \n",
       "1           1  Define\\n\\[p = \\sum_{k = 1}^\\infty \\frac{1}{k^2...   \n",
       "\n",
       "                                            solution  \\\n",
       "0  We have that $r = \\sqrt{0^2 + 3^2} = 3.$  Also...   \n",
       "1  We count the number of times $\\frac{1}{n^3}$ a...   \n",
       "\n",
       "                            answer               subject  level  \\\n",
       "0  \\left( 3, \\frac{\\pi}{2} \\right)           Precalculus      2   \n",
       "1                            p - q  Intermediate Algebra      5   \n",
       "\n",
       "                             unique_id  \n",
       "0            test/precalculus/807.json  \n",
       "1  test/intermediate_algebra/1994.json  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import litellm\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from judge import Judge\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# Optional: model aliases\n",
    "litellm.model_alias_map[\"openai/gpt-4o\"] = \"gpt-4o\"\n",
    "litellm.model_alias_map[\"gemini/flash-1.5\"] = \"gemini/gemini-1.5-flash\"\n",
    "\n",
    "#load the dataset\n",
    "df_100 = pd.read_csv(\"../output_data/100_data_math500.csv\")\n",
    "# Prompts\n",
    "COT_PROMPT = \"\"\"\n",
    "You are a math tutor helping a student. Solve the following math problem step-by-step.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Let's think step-by-step.\n",
    "\"\"\"\n",
    "\n",
    "REFLECTION_PROMPT = \"\"\"\n",
    "Now reflect on the solution above. Was the reasoning and final answer correct? If there was a mistake, explain it and provide a corrected solution.\n",
    "\"\"\"\n",
    "judge = Judge(model='gemini/gemini-2.0-flash')\n",
    "\n",
    "df_100.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [27:17, 16.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            question  \\\n",
      "0  Convert the point $(0,3)$ in rectangular coord...   \n",
      "1  Define\\n\\[p = \\sum_{k = 1}^\\infty \\frac{1}{k^2...   \n",
      "2  If $f(x) = \\frac{3x-2}{x-2}$, what is the valu...   \n",
      "3  How many positive whole-number divisors does 1...   \n",
      "4  The results of a cross-country team's training...   \n",
      "\n",
      "                      ground_truth  \\\n",
      "0  \\left( 3, \\frac{\\pi}{2} \\right)   \n",
      "1                            p - q   \n",
      "2                     \\frac{14}{3}   \n",
      "3                                9   \n",
      "4                    \\text{Evelyn}   \n",
      "\n",
      "                                          cot_answer  \\\n",
      "0  To convert the point \\((0, 3)\\) from rectangul...   \n",
      "1  To solve this problem, we need to express the ...   \n",
      "2  To solve the problem, we need to evaluate the ...   \n",
      "3  To find the number of positive whole-number di...   \n",
      "4  To determine which student has the greatest av...   \n",
      "\n",
      "                                    reflected_answer  result  \n",
      "0  The solution provided is correct. Let's review...    True  \n",
      "1  The reasoning and final answer provided in the...    True  \n",
      "2  The solution provided is correct. Let's review...    True  \n",
      "3  The solution provided is correct. Let's review...    True  \n",
      "4  The reasoning and calculations for determining...    True  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# LLM Call\n",
    "def call_litellm(prompt, model=\"openai/gpt-4o\"):\n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "for _,item in tqdm(df_100.iterrows()):\n",
    "    q = item[\"problem\"]\n",
    "    truth = item[\"answer\"]\n",
    "\n",
    "    # Chain of Thought\n",
    "    cot_prompt = COT_PROMPT.format(question=q)\n",
    "    cot_response = call_litellm(cot_prompt,model=\"openai/gpt-4o\")\n",
    "\n",
    "    # Self Reflection\n",
    "    reflection_prompt = cot_response + \"\\n\\n\" + REFLECTION_PROMPT\n",
    "    reflection_response = call_litellm(reflection_prompt,model=\"openai/gpt-4o\")\n",
    "\n",
    "    result = judge.prediction(query=q,answer1=reflection_response,answer2=truth).correct\n",
    "\n",
    "    rows.append({\n",
    "        \"question\": q,\n",
    "        \"ground_truth\": truth,\n",
    "        \"cot_answer\": cot_response,\n",
    "        \"reflected_answer\": reflection_response,\n",
    "        \"result\": result\n",
    "    })\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Save or display\n",
    "df.to_csv(\"../output_data/math500_cot_reflection_output_gpt_4o.csv\", index=False)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Call\n",
    "def call_litellm(prompt, model=\"openai/gpt-4o\"):\n",
    "    response = litellm.completion(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "\n",
    "rows = []\n",
    "\n",
    "for _,item in tqdm(df_100.iterrows()):\n",
    "    q = item[\"problem\"]\n",
    "    truth = item[\"answer\"]\n",
    "\n",
    "    # Chain of Thought\n",
    "    cot_prompt = COT_PROMPT.format(question=q)\n",
    "    cot_response = call_litellm(cot_prompt,model=\"gemini/gemini-1.5-flash\")\n",
    "\n",
    "    # Self Reflection\n",
    "    reflection_prompt = cot_response + \"\\n\\n\" + REFLECTION_PROMPT\n",
    "    reflection_response = call_litellm(reflection_prompt,model=\"gemini/gemini-1.5-flash\")\n",
    "\n",
    "    result = judge.prediction(query=q,answer1=reflection_response,answer2=truth).correct\n",
    "\n",
    "    rows.append({\n",
    "        \"question\": q,\n",
    "        \"ground_truth\": truth,\n",
    "        \"cot_answer\": cot_response,\n",
    "        \"reflected_answer\": reflection_response,\n",
    "        \"result\": result\n",
    "    })\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# Save or display\n",
    "df.to_csv(\"../output_data/math500_cot_reflection_output_gemini_1_5_flash.csv\", index=False)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result\n",
       "True     81\n",
       "False    19\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result\n",
       "True     80\n",
       "False    20\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gpt4o = pd.read_csv(\"../output_data/math500_cot_reflection_output_gpt_4o.csv\")\n",
    "df_gpt4o['result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".global-uv-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
